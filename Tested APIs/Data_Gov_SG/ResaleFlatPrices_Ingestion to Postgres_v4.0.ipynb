{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates in v3.0\n",
    "# from v2.0, uses \"_links\", \"start\" and \"next\" at the bottom of JSON file to get the next link and page\n",
    "# can test for smallet dataset with max_pages=10\n",
    "\n",
    "# Updates in v4.0\n",
    "# removed create_table function\n",
    "# edited column names to fit postgreSQL table design\n",
    "# created resale_id -> combined_df['resale_id'] = range(1, len(combined_df) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in /opt/anaconda3/lib/python3.12/site-packages (2.0.30)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from sqlalchemy) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from sqlalchemy) (3.0.1)\n",
      "Requirement already satisfied: psycopg2 in /opt/anaconda3/lib/python3.12/site-packages (2.9.10)\n",
      "Collecting sqlalchemy_utils\n",
      "  Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.3 in /opt/anaconda3/lib/python3.12/site-packages (from sqlalchemy_utils) (2.0.30)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy>=1.3->sqlalchemy_utils) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy>=1.3->sqlalchemy_utils) (3.0.1)\n",
      "Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sqlalchemy_utils\n",
      "Successfully installed sqlalchemy_utils-0.41.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy\n",
    "!pip install psycopg2\n",
    "!pip install sqlalchemy_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the script...\n",
      "Fetched data frames: 500\n",
      "Successfully loaded 500 records to PostgreSQL.\n",
      "Total records in 'resale_flat_txn' table: 500\n",
      "Script completed.\n"
     ]
    }
   ],
   "source": [
    "import requests  # For sending HTTP requests\n",
    "import pandas as pd  # For data manipulation\n",
    "from datetime import datetime, timedelta  # For handling dates\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sqlalchemy import create_engine, text  # For database operations\n",
    "\n",
    "# Define constants\n",
    "API_URL = \"https://data.gov.sg/api/action/datastore_search?resource_id=d_8b84c4ee58e3cfc0ece0d773c8ca6abc\"  # API URL\n",
    "DB_USER = 'postgres'  # Update with your PostgreSQL username\n",
    "DB_PASS = 'admin'  # Update with your PostgreSQL password\n",
    "DB_HOST = 'localhost'  # Update with your database host\n",
    "DB_PORT = '5432'  # Update with your database port\n",
    "DB_NAME = 'data_gov_project'  # Update with your PostgreSQL database name\n",
    "# START_DATE = datetime(2024, 9, 1)  # Adjust as needed (yyyy, mm, dd, hh, mm)\n",
    "# END_DATE = datetime(2024, 10, 31)  # Adjust as needed (yyyy, mm, dd, hh, mm)\n",
    "\n",
    "def fetch_data_from_api(api_url, max_pages=5):\n",
    "    \"\"\"Fetch data from the API, stopping after a set number of pages for testing.\"\"\"\n",
    "    data_frames = []  # List to store DataFrames for each batch of data\n",
    "    next_url = api_url\n",
    "    page_count = 0  # Track the number of pages fetched\n",
    "\n",
    "    while next_url and page_count < max_pages:\n",
    "        response = requests.get(next_url)\n",
    "        if response.status_code == 200:\n",
    "            json_data = response.json()\n",
    "            records = json_data.get(\"result\", {}).get(\"records\", [])\n",
    "            if records:\n",
    "                process_items(records, data_frames)\n",
    "\n",
    "            # Check for the next page link\n",
    "            next_link = json_data.get(\"result\", {}).get(\"_links\", {}).get(\"next\")\n",
    "            if next_link:\n",
    "                next_url = f\"https://data.gov.sg{next_link}\"\n",
    "            else:\n",
    "                next_url = None\n",
    "\n",
    "            page_count += 1  # Increment page count\n",
    "        else:\n",
    "            print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "            next_url = None\n",
    "\n",
    "    return data_frames\n",
    "    \n",
    "def process_items(items, data_frames):\n",
    "    \"\"\"Process the fetched items and store data in DataFrames.\"\"\"\n",
    "    for item in items:\n",
    "        # Extract relevant fields from the item\n",
    "        record = {\n",
    "            'resale_date': item.get('month'),\n",
    "            'town_name': item.get('town'),\n",
    "            'flat_type': item.get('flat_type'),\n",
    "            'block_no': item.get('block'),\n",
    "            'street_name': item.get('street_name'),\n",
    "            'storey_range': item.get('storey_range'),\n",
    "            'floor_area_sqm': item.get('floor_area_sqm'),\n",
    "            'flat_model': item.get('flat_model'),\n",
    "            'lease_commence_year': item.get('lease_commence_date'),\n",
    "            'remaining_lease': item.get('remaining_lease'),\n",
    "            'resale_price': item.get('resale_price'),\n",
    "        }\n",
    "\n",
    "        # Create a DataFrame from the record and append it to the list\n",
    "        df = pd.DataFrame([record])\n",
    "        data_frames.append(df)\n",
    "\n",
    "def load_data_to_postgres(data_frame):\n",
    "    \"\"\"Load the provided DataFrame into the PostgreSQL database.\"\"\"\n",
    "    engine = create_engine(f'postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}')\n",
    "    try:\n",
    "        data_frame.to_sql('resale_flat_txn', engine, if_exists='append', index=False)\n",
    "        print(f\"Successfully loaded {len(data_frame)} records to PostgreSQL.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data into PostgreSQL: {e}\")\n",
    "\n",
    "def verify_data_in_db():\n",
    "    \"\"\"Verifies the data in the PostgreSQL database.\"\"\"\n",
    "    engine = create_engine(f'postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}')\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            result = connection.execute(text(\"SELECT COUNT(*) FROM resale_flat_txn\"))\n",
    "            count = result.fetchone()[0]\n",
    "            print(f\"Total records in 'resale_flat_txn' table: {count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying data in PostgreSQL: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the script.\"\"\"\n",
    "    print(\"Starting the script...\")\n",
    "\n",
    "    data_frames = fetch_data_from_api(API_URL)  # Fetch the data\n",
    "    print(f\"Fetched data frames: {len(data_frames)}\")\n",
    "\n",
    "    if data_frames:\n",
    "        combined_df = pd.concat(data_frames, ignore_index=True)  # Combine the DataFrames\n",
    "\n",
    "        # Clean up DataFrame (if necessary)\n",
    "        combined_df['resale_date'] = pd.to_datetime(combined_df['resale_date'], errors='coerce')  # Convert month to datetime\n",
    "        combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]  # Remove duplicates\n",
    "        combined_df = combined_df[['resale_date', 'town_name', 'flat_type', 'block_no', 'street_name', \n",
    "                                   'storey_range', 'floor_area_sqm', 'flat_model', 'lease_commence_year', \n",
    "                                   'remaining_lease', 'resale_price']]  # Rearrange columns\n",
    "\n",
    "        combined_df['resale_id'] = range(1, len(combined_df) + 1)\n",
    "        load_data_to_postgres(combined_df)  # Load the data into PostgreSQL\n",
    "\n",
    "        verify_data_in_db()  # Verify the data in the database\n",
    "    else:\n",
    "        print(\"No data collected.\")\n",
    "    \n",
    "    print(\"Script completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
