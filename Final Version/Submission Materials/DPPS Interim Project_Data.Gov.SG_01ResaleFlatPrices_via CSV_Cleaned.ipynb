{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ef722e-2d82-4fda-a179-e5a60ca551e4",
   "metadata": {},
   "source": [
    "# **Generation SG Junior Data Engineer Programme**\n",
    "### **Interim Project presented by DPPS Team (5)**<br><span style=\"color:darkblue; font-weight:bold;\">Members: Daniel | Pin Pin, Yvonne | Pin Yean, Erica | Shawn</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkblue; font-weight:bold;\">Singapore Resale Flat Prices: Comprehensive Data Ingestion Methodology</span>\n",
    "<div>This documentation outlines our systematic approach to ingesting and processing the Singapore Housing & Development Board (HDB) resale flat transaction dataset. Our robust data pipeline transforms raw CSV data into a structured PostgreSQL database, enabling comprehensive real estate market analysis. Our data ingestion workflow:</div>\n",
    "\n",
    "- **Database Schema Preparation**: developed a precise PostgreSQL schema to capture the nuanced details of HDB resale transactions\n",
    "- **Data Source Acquisition**: covers entire Singapore residential resale market\n",
    "- **Data Transformation and Loading**: the Python-based data ingestion script implements a sophisticated ETL (Extract, Transform, Load) process\n",
    "\n",
    "The resulting database provides a robust foundation for advanced real estate market analysis, enabling precise insights into Singapore's dynamic resale flat ecosystem.\n",
    "\n",
    "**[Link](https://data.gov.sg/datasets/d_8b84c4ee58e3cfc0ece0d773c8ca6abc/view)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da27b4-b8e8-4be4-9660-5f6eb87fd990",
   "metadata": {},
   "source": [
    "### **API vs. CSV Data Ingestion: Strategic Decision for Singapore Resale Flat Transactions**\n",
    "While developing our data pipeline for Singapore real estate market transactions, we conducted a comprehensive comparative analysis between API and CSV ingestion methodologies. Our objective was to optimize data retrieval efficiency for the daily-updated HDB resale flat transaction dataset.\n",
    "\n",
    "**Key Challenges with API-Based Ingestion:**\n",
    "1. **Limited Temporal Filtering Capabilities**: the API presented significant constraints in temporal data segmentation. Despite successfully implementing pagination strategies for JSON file extraction, we encountered critical limitations:\n",
    "    - Inability to specify precise start and end dates\n",
    "    - Restricted control over transaction and price data time periods\n",
    "    - Reduced granularity in data retrieval\n",
    "2. **Performance Bottlenecks**: performance metrics revealed stark disparities between API and CSV ingestion approaches:\n",
    "    - API Ingestion: 15 minutes processing time\n",
    "    - CSV Ingestion: 10 seconds processing time\n",
    "    - Performance Acceleration: Approximately **8,900%** speed enhancement\n",
    "\n",
    "**Strategic Decision on CSV-Based Data Ingestion**: given these technical constraints, we strategically pivoted to CSV-based ingestion, enabling:\n",
    "- Comprehensive data coverage from January 2017 to Present (November 2024)\n",
    "- Instantaneous full dataset loading\n",
    "- Enhanced data retrieval flexibility\n",
    "- Minimal computational overhead\n",
    "\n",
    "**Technical Rationale**\n",
    "- Maximized data acquisition efficiency\n",
    "- Simplified data pipeline architecture\n",
    "- Reduced computational resource consumption\n",
    "- Improved overall system responsiveness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520be32-3bc3-45bd-8b79-7420d6fae3cf",
   "metadata": {},
   "source": [
    "### **Python Libraries: Our Comprehensive Data Engineering Toolkit**\n",
    "Our meticulously curated Python library selection represents a strategic approach to building a robust, scalable data processing ecosystem. Each library was deliberately chosen to address specific technical challenges in our data engineering workflow.\n",
    "\n",
    "- **Pandas**: used for data manipulation for effectively handling structured data\n",
    "- **SQLAlchemy**: a SQL toolkit and Object-Relational Mapping (ORM) to connect to relational databases\n",
    "- **psycopg2**: PostgreSQL database adapter\n",
    "\n",
    "**Strategic Library Synergies**\n",
    "- Comprehensive data processing capabilities\n",
    "- Seamless database interaction\n",
    "- High-performance computational infrastructure\n",
    "- Scalable and flexible data engineering architecture\n",
    "\n",
    "By integrating these libraries, we've created a powerful, flexible toolkit capable of handling complex data engineering challenges with exceptional efficiency and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea657ed-a3e8-4b0d-8aa2-94734fd2a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the following packages in your Anaconda Prompt or Terminal:\n",
    "conda install request\n",
    "conda install pandas\n",
    "conda install sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035cff69-a40f-4583-8628-0a30d2e54306",
   "metadata": {},
   "source": [
    "### **Hard Coding vs Function-based Code**\n",
    "We started with \"Hard Coding\", where the script was just 1 long sequence of codes. However, as we realised that we would repeatedly reuse this ingestion script over a few times, we moved to a function-based code. \n",
    "Doing so, allowed us to enjoy the below benefits:\n",
    "\n",
    "1. **Reusable Logic**: The function can be used in multiple places with different inputs.\n",
    "2. **Modular and Scalable**: Encourages separation of concerns, making the code easier to understand and modify.\n",
    "3. **Flexible**: Parameters allow different values without altering the source code.\n",
    "4. **Easier to Test**: Isolating logic in functions simplifies debugging and unit testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd242f-524d-43a0-8340-ff8eb863c8b5",
   "metadata": {},
   "source": [
    "### **Function Sequence**\n",
    "Here would be a description of what each function does and how it flows:\n",
    "\n",
    "1. Begin by defining the constant variables used in the script: csv_file_path, db_user, db_pass, db_host, db_port and db_name  \n",
    "1. load_data_from_csv(file_path): loads the data from file path, and filters by date range, using pandas .read_csv() and dataframes \n",
    "2. load_data_to_postgres(data_frame): connects to postgreSQL database, and inserts into the database using libraries sqlalchemy and psycopg2\n",
    "3. main(): brings the above 3 together in 1 elegant function, and prints outputs to update the user on progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed24386a",
   "metadata": {},
   "source": [
    "### **Optimized Data Ingestion: Resale Flat Transaction CSV Ingestion**\n",
    "Our advanced data acquisition script represents a breakthrough in computational efficiency, dramatically reducing data retrieval time by **8900%** â€” transforming a protracted 15-minute process into a swift 10-second operation. This sophisticated engineering solution delivers a comprehensive approach to temperature data collection, processing, and storage.\n",
    "\n",
    "**Technical Architecture: Key Performance Capabilities**\n",
    "1. **Advanced Data Collection Strategy**: systematic CSV-based data retrieval for comprehensive resale flat transaction records\n",
    "2. **Efficient Data Processing:**: leverages pandas for sophisticated data manipulation and transformation\n",
    "3. **Intelligent Error Management**: robust try-except error handling mechanism\n",
    "4. **Data Quality Assurance**: proactive data cleaning and preprocessing techniques\n",
    "5. **Architectural Design Principles**: modular function-based architecture, enhances code readability and maintainability\n",
    "6. **Database Integration Capabilities**: seamless PostgreSQL data persistence, utilizes SQLAlchemy for advanced database connectivity\n",
    "7. **Flexible Data Acquisition Framework**: supports comprehensive historical data retrieval, enable flexible data loading from CSV sources\n",
    "8. **Advanced Data Processing**: standardizes raw data into consistent database schema, prepares data for analytical processing\n",
    "9. **Enterprise-Grade Scalability**: handles large\n",
    "\n",
    "**Library Ecosystem**\n",
    "- **Pandas**: Core data manipulation and transformation\n",
    "- **SQLAlchemy**: Advanced database connectivity and ORM\n",
    "- **Psycopg2**: Low-level PostgreSQL database interactions\n",
    "- **Psycopg2.extras**: Efficient bulk data insertion capabilities\n",
    "\n",
    "The script transcends traditional data retrieval approaches, offering a robust, intelligent solution for comprehensive environmental data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da47549-8556-4e83-8af0-b9bf26d2dabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the script...\n",
      "Filtered records count: 27656\n",
      "Unique resale prices: [288000. 265000. 378000. ... 610599. 752888. 855500.]\n",
      "Successfully loaded 27656 records to PostgreSQL.\n",
      "Script completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # For data manipulation\n",
    "from sqlalchemy import create_engine # For database connectivity\n",
    "import psycopg2 # For PostgreSQL connectivity\n",
    "from psycopg2 import sql # For SQL queries\n",
    "from psycopg2.extras import execute_values # For bulk insert\n",
    "\n",
    "# Define constants\n",
    "CSV_FILE_PATH = '/Users/shawnwee/teams notes_Generation SCTP JDE 05/Week 5 Interim Project/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv' # Update your file path\n",
    "DB_USER = 'postgres'             # Update with your PostgreSQL username\n",
    "DB_PASS = 'password'             # Update with your PostgreSQL password\n",
    "DB_HOST = 'localhost'            # Update with your database host\n",
    "DB_PORT = '5432'                 # Update with your database port\n",
    "DB_NAME = 'data_gov_project'     # Update with your PostgreSQL database name\n",
    "\n",
    "# Define date range for filtering\n",
    "START_DATE = pd.Timestamp('2023-12-01')\n",
    "END_DATE = pd.Timestamp('2024-11-30')\n",
    "\n",
    "def load_data_from_csv(file_path):\n",
    "    \"\"\"Load data from a CSV file and filter by date range.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert 'month' to a datetime representing resale_date\n",
    "    df['resale_date'] = pd.to_datetime(df['month'], format='%Y-%m', errors='coerce')\n",
    "\n",
    "    # Convert numeric columns to appropriate types\n",
    "    df['floor_area_sqm'] = pd.to_numeric(df['floor_area_sqm'], errors='coerce')\n",
    "    df['resale_price'] = pd.to_numeric(df['resale_price'], errors='coerce')\n",
    "    df['lease_commence_year'] = pd.to_datetime(df['lease_commence_date'], errors='coerce').dt.year\n",
    "\n",
    "    # Prepare DataFrame and rename columns for PostgreSQL compatibility\n",
    "    processed_df = df.rename(columns={\n",
    "        'town': 'town_name',\n",
    "        'block': 'block_no',\n",
    "        'storey_range': 'storey_range'  # This remains unchanged\n",
    "    })\n",
    "\n",
    "    # Select relevant columns for PostgreSQL, excluding 'resale_id'\n",
    "    processed_df = processed_df[['resale_date', 'town_name', 'flat_type', 'block_no', \n",
    "                                  'street_name', 'storey_range', 'floor_area_sqm', \n",
    "                                  'flat_model', 'lease_commence_year', 'remaining_lease', \n",
    "                                  'resale_price']]\n",
    "    \n",
    "    # Filter for valid date ranges\n",
    "    filtered_df = processed_df[(processed_df['resale_date'] >= START_DATE) & \n",
    "                                (processed_df['resale_date'] <= END_DATE)]\n",
    "\n",
    "    print(f\"Filtered records count: {len(filtered_df)}\")\n",
    "    print(\"Unique resale prices:\", filtered_df['resale_price'].unique())\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def load_data_to_postgres(data_frame):\n",
    "    \"\"\"Load the provided DataFrame into the PostgreSQL database.\"\"\"\n",
    "    # Create database engine for SQLAlchemy usage\n",
    "    engine = create_engine(f'postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}')\n",
    "\n",
    "    # Prepare for bulk insert using psycopg2\n",
    "    conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Prepare the insert query\n",
    "    insert_query = sql.SQL(\"\"\"\n",
    "        INSERT INTO resale_flat_txn (resale_date, town_name, flat_type, block_no, street_name, \n",
    "                                      storey_range, floor_area_sqm, flat_model, lease_commence_year, \n",
    "                                      remaining_lease, resale_price)\n",
    "        VALUES %s\n",
    "    \"\"\")\n",
    "\n",
    "    # Prepare tuples for the insert query\n",
    "    data_tuples = [tuple(x) for x in data_frame.values]\n",
    "\n",
    "    try:\n",
    "        # Insert in batches using execute_values\n",
    "        execute_values(cur, insert_query, data_tuples, template=None, page_size=1000)\n",
    "        conn.commit()\n",
    "        print(f\"Successfully loaded {len(data_tuples)} records to PostgreSQL.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data into PostgreSQL: {e}\")\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the script.\"\"\"\n",
    "    print(\"Starting the script...\")\n",
    "    \n",
    "    # Load the data from CSV\n",
    "    filtered_df = load_data_from_csv(CSV_FILE_PATH)\n",
    "    \n",
    "    # Load the filtered data into PostgreSQL\n",
    "    load_data_to_postgres(filtered_df)\n",
    "\n",
    "    print(\"Script completed.\")\n",
    "\n",
    "# Execute the script\n",
    "if __name__ == \"__main__\": # Running main function\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f3fe2-7a23-4568-93be-61b38e304cbb",
   "metadata": {},
   "source": [
    "### **Resale Flat transaction Output Result**\n",
    "<img src=\"https://raw.githubusercontent.com/YvonneLipLim/Images/main/Resale_Flat_Output.png\" alt=\"Alt Text\" width=\"800\">\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
