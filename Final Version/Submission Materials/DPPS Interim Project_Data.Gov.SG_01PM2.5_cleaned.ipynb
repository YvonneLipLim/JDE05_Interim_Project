{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ef722e-2d82-4fda-a179-e5a60ca551e4",
   "metadata": {},
   "source": [
    "# **Generation SG Junior Data Engineer Programme**\n",
    "### **Interim Project presented by DPPS Team (5)**<br><span style=\"color:darkblue; font-weight:bold;\">Members: Daniel | Pin Pin, Yvonne | Pin Yean, Erica | Shawn</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkblue; font-weight:bold;\">API Data Extraction - PM2.5 across Singapore</span>\n",
    "<div>This document demonstrated how the team made requests and worked with APIs in Python using common libraries to extract a single source of truth data from OpenAPI, Data.Gov.SG for our project that offers the following benefits:</div>\n",
    "\n",
    "- **Real-time data access**: retrieve up-to-date data on demand\n",
    "- **Large datasets**: integrate data from multiple sources\n",
    "- **Pre-processed data**: saving significant time and resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f426c0ba-9db7-4cf4-9d8a-bbcd380752f7",
   "metadata": {},
   "source": [
    "### **Understanding API Documentation**\n",
    "When working with APIs, consulting the documentation is crucial for making successful requests. It includes:\n",
    "- available endpoints\n",
    "- required parameters\n",
    "- authentication methods\n",
    "- expected response formats\n",
    "- license of the Open Data (free or paid, personal or commercial, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb98a0c-4662-48da-8e96-6c2b5ecaa6c5",
   "metadata": {},
   "source": [
    "### **Evaluating The Options Available**\n",
    "<div>We accessed different APIs in Data.Gov.SG and noticed it uses <em>REST API</em> (Representational State Transfer Application Programming Interface), a popular web service that follows the <em>REST</em> architectural style, allowing applications to communicate with each other by exchanging data through standardized methods, typically using HTTP requests to access and manipulate resources on a server.</div><br>\n",
    "\n",
    "**API request and response models:**\n",
    "- **Request body**: JSON (JavaScript Object Notation) format\n",
    "- **200**: Everything went okay and the result has been returned (if any)\n",
    "- **400**: Server thinks we made a bad request when we send incorrect data or make other client-side errors\n",
    "- **404**: Resource we tried to access wasn't found on server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97c15c-bbca-482c-8946-1926d8140e09",
   "metadata": {},
   "source": [
    "### **Making GET Request To The API**\n",
    "<div>We initiated a <b>GET Request</b> to the Singapore government's open data API endpoint, specifically targeting  real-time PM2.5 atmospheric particulate data. The retrieved JSON response provided comprehensive information about microscopic particles and droplets measuring 2.5 micrometres or less in diameter, captured across multiple monitoring locations. The acquired dataset enables detailed analysis of:</div>\n",
    "\n",
    "1. The comprehensive network of air quality monitoring stations, including their total number and strategic placement\n",
    "2. Precise geographical distribution of monitoring infrastructure\n",
    "3. Instantaneous PM2.5 concentration readings for each monitored location\n",
    "4. Exact timestamps associated with each measurement\n",
    "\n",
    "By meticulously examining this JSON dataset, we can derive critical insights into the sophistication and extensive coverage of Singapore's environmental monitoring system, revealing the intricate dynamics of airborne particulate matter across the city-state's diverse urban and suburban landscapes.\n",
    "\n",
    "**[Link](https://api-open.data.gov.sg/v2/real-time/api/pm25)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fbba39",
   "metadata": {},
   "source": [
    "<span style=\"color:darkgreen; font-weight:bold;\">JSON code</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e492fe2",
   "metadata": {
    "hide_input": false,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"code\": 0,\n",
    "  \"data\": {\n",
    "    \"regionMetadata\": [\n",
    "      {\n",
    "        \"name\": \"west\",\n",
    "        \"labelLocation\": {\n",
    "          \"latitude\": 1.35735,\n",
    "          \"longitude\": 103.7\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"east\",\n",
    "        \"labelLocation\": {\n",
    "          \"latitude\": 1.35735,\n",
    "          \"longitude\": 103.94\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"central\",\n",
    "        \"labelLocation\": {\n",
    "          \"latitude\": 1.35735,\n",
    "          \"longitude\": 103.82\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"south\",\n",
    "        \"labelLocation\": {\n",
    "          \"latitude\": 1.29587,\n",
    "          \"longitude\": 103.82\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"north\",\n",
    "        \"labelLocation\": {\n",
    "          \"latitude\": 1.41803,\n",
    "          \"longitude\": 103.82\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"items\": [\n",
    "      {\n",
    "        \"date\": \"2024-11-30\",\n",
    "        \"updatedTimestamp\": \"2024-11-30T21:15:41+08:00\",\n",
    "        \"timestamp\": \"2024-11-30T21:00:00+08:00\",\n",
    "        \"readings\": {\n",
    "          \"pm25_one_hourly\": {\n",
    "            \"west\": 6,\n",
    "            \"east\": 11,\n",
    "            \"central\": 24,\n",
    "            \"south\": 12,\n",
    "            \"north\": 9\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"errorMsg\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520be32-3bc3-45bd-8b79-7420d6fae3cf",
   "metadata": {},
   "source": [
    "### **Python Libraries Used**\n",
    "We selected these libraries together as it provide a powerful toolkit for data processing, web interaction, database operations and parallel execution in Python.\n",
    "- **Requests**: used for making HTTP requests in Python\n",
    "- **Pandas**: used for data manipulation for effectively handling structured data\n",
    "- **Datetime and Timedelta**: used for working with dates, times and time intervals\n",
    "- **SQLAlchemy**: a SQL toolkit and Object-Relational Mapping (ORM) to connect to relational databases\n",
    "- **Logging**: provides a flexible framework for generating log messages\n",
    "- **Concurrent.futures**: used for parallelizing tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875bc2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the following packages in your Anaconda Prompt or Terminal:\n",
    "conda install request\n",
    "conda install pandas\n",
    "conda install sqlalchemy\n",
    "\n",
    "# Datetime, Logging and Concurrent Futures are standard libraries included"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed24386a",
   "metadata": {},
   "source": [
    "### **Accessing Data**\n",
    "During the development of our project's next phase, we designed a specialized script to facilitate robust API data ingestion. This script represents a critical component of our data pipeline, enabling efficient and reliable extraction of essential project information through systematic API interaction.\n",
    "\n",
    "Key Architectural Advantages:\n",
    "1. **Comprehensive Data Collection**: Systematically retrieves hourly air temperature data, enabling in-depth analytical capabilities\n",
    "2. **Parallel Processing Architecture**: Utilizes ThreadPoolExecutor to facilitate concurrent data fetching, substantially minimizing overall execution time\n",
    "3. **Robust Error Management**: Implements sophisticated error handling through try-except mechanisms, providing granular logging and debugging capabilities\n",
    "4. **Data Integrity Enforcement**: Eliminates duplicate entries proactively, ensuring clean and accurate datasets\n",
    "5. **Modular and Maintainable Design**: Structures code with distinct, purpose-driven functions to enhance readability and long-term maintainability\n",
    "6. **Seamless Database Integration**: Efficiently stores processed data in PostgreSQL, maintaining optimal data persistence\n",
    "7. **Comprehensive Data Validation**: Conducts post-insertion record verification for immediate data quality assurance\n",
    "8. **Flexible Date Range Processing**: Supports configurable start and end dates for precise data collection periods\n",
    "9. **Advanced Data Transformation**: Leverages pandas for sophisticated data manipulation and structural refinement\n",
    "10. **API Interaction Resilience**: Gracefully manages potential API failures through intelligent error capture and logging\n",
    "11. **Enterprise-Grade Scalability**: Designed to effortlessly handle extensive date ranges and high-frequency data acquisition\n",
    "12. **Universal Adaptability**: Easily customizable for diverse API data collection scenarios\n",
    "13. **Automated Workflow Capabilities**: Enables autonomous, scheduled data collection and storage processes\n",
    "14. **Standardized Data Normalization**: Transforms raw API responses into consistent, structured formats before database insertion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addbab9-8920-4a20-8f80-d99e011f4c91",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Constants Definition\n",
    "API_URL = \"https://api-open.data.gov.sg/v2/real-time/api/pm25\"\n",
    "DB_USER = 'postgres'            # Database username\n",
    "DB_PASS = 'password'            # Database password\n",
    "DB_HOST = 'localhost'           # Database server address\n",
    "DB_PORT = '5432'                # Database port number, default for Postgre\n",
    "DB_NAME = 'data_gov_project_2'  # Name of database for connection\n",
    "START_DATE = datetime(2024, 11, 1)  # Adjust to one year ago (yyyy, mm, dd)\n",
    "END_DATE = datetime(2024, 11, 7)  # Today's date or desired end date (yyyy, mm, dd)\n",
    "\n",
    "def fetch_pm25_data_for_date(api_url, date):\n",
    "    \"\"\"Fetch PM2.5 data for a specific date.\"\"\"\n",
    "    try: # Handle potential errors during data fetching\n",
    "        response = requests.get(f\"{api_url}?date={date}\") # Performs a GET request appending date\n",
    "        if response.status_code == 200: # Checks the response status\n",
    "            json_data = response.json() # If request is successful, response id parsed as JSON\n",
    "            items = json_data.get(\"data\", {}).get(\"items\", []) # Retrieves items key from data section of JSON. Return empty list if it doesn't exist\n",
    "            region_metadata = json_data.get(\"data\", {}).get(\"regionMetadata\", []) # Retrieves metadata \n",
    "            return items, region_metadata # If data is fetched successfully, the function returns a tuple containing items and region metadata\n",
    "        # If the status code is not 200, an error message is printed indicating failure and function returns 'None' for both values\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for {date}. Status code: {response.status_code}\")\n",
    "            return None, None\n",
    "    # Catches any exceptions that may occur during request or data processing, prints an error message and returns 'None' for both values\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {date}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def process_items(items, region_metadata):\n",
    "    \"\"\"Process the fetched items and store PM2.5 readings in a DataFrame.\"\"\"\n",
    "    data_frames = [] # An empty list to store DataFrames created for each item\n",
    "    for item in items: # Iterates through each item in the items list\n",
    "        timestamp = item.get('timestamp') # Timestamp for each item is extracted\n",
    "        readings = item.get('readings', {}).get('pm25_one_hourly', {}) # Hourly readings are extracted, default to empty dictionary if don't exist\n",
    "        \n",
    "        for region, value in readings.items(): # Loop processes each region PM2.5 reading\n",
    "            # Find region metadata\n",
    "            region_info = next((r for r in region_metadata if r['name'].lower() == region.lower()), {}) # Extract metadata based on its name, case-insensitively\n",
    "            latitude = region_info.get('labelLocation', {}).get('latitude') # Extract latitude\n",
    "            longitude = region_info.get('labelLocation', {}).get('longitude') # Extract longtitude\n",
    "\n",
    "            # Create a DataFrame for the reading\n",
    "            df = pd.DataFrame({ # A new DataFrame is created for the following columns:\n",
    "                'region': [region],\n",
    "                'pm25_value': [value],\n",
    "                'timestamp': [timestamp],\n",
    "                'latitude': [latitude],\n",
    "                'longitude': [longitude]\n",
    "            })\n",
    "            data_frames.append(df) # The newly DataFrame is appended to the data_frames list\n",
    "\n",
    "    if data_frames: # Checks id any DataFrames were created\n",
    "        return pd.concat(data_frames, ignore_index=True) # Concatenated into a single DataFrame and index is reset if there are DataFrame present\n",
    "    else:\n",
    "        return pd.DataFrame() # If no DaaFrames were created, an empty DataFrame is returned\n",
    "\n",
    "def load_data_to_postgres(data_frame):\n",
    "    \"\"\"Load the provided DataFrame into the PostgreSQL database.\"\"\"\n",
    "    engine = create_engine(f'postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}') # Engine for connect PostgreSQL database\n",
    "    try: # Handle potential errors that might occur during the data loading process\n",
    "        data_frame.to_sql('pm25_readings', engine, if_exists='append', index=False) # Use Pandas method to load contents of DataFrame into table\n",
    "        print(f\"Successfully loaded {len(data_frame)} records to PostgreSQL.\") # Print records if loading successful\n",
    "    except Exception as e: # To catch any exceptions that may occur during the loading process\n",
    "        print(f\"Error loading data into PostgreSQL: {e}\")\n",
    "\n",
    "def verify_data_in_db():\n",
    "    \"\"\"\n",
    "    Retrieves number of rows from 'pm25_readings' table to verify data was loaded successfully.\n",
    "    \"\"\"\n",
    "    engine = create_engine(f'postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}') # Engine to connect to PostgreSQL databas\n",
    "    try: # Handle potential errors that might occur during the data loading process\n",
    "        with engine.connect() as connection: # Ensure connection is properly handled and closed after use\n",
    "            result = connection.execute(text(\"SELECT COUNT(*) FROM pm25_readings\")) # Executes SQL query to count total number of rows\n",
    "            count = result.fetchone()[0] # Retrieves first row result\n",
    "            print(f\"Total records in 'pm25_readings' table: {count}\")  # Show count of rows\n",
    "    except Exception as e: # To catch any exceptions that may occur during the loading process\n",
    "        print(f\"Error verifying data in PostgreSQL: {e}\")\n",
    "\n",
    "def verify_database_connection():\n",
    "    \"\"\"Verifies the database connection by printing a message if successful.\"\"\"\n",
    "    engine = create_engine(f'postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}') # Engine for connect PostgreSQL database\n",
    "    try: # Handle potential errors that might occur during the data loading process\n",
    "        with engine.connect() as connection: # Ensure connection is properly handled and closed after use\n",
    "            print(\"Database connection successful\")\n",
    "    except Exception as e: # To catch any exceptions that may occur during the loading process\n",
    "        print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "\n",
    "# Function is created in view that SQL database has not created the table\n",
    "def create_table():\n",
    "    \"\"\"Create the 'pm25_readings' table in the database.\"\"\"\n",
    "    engine = create_engine(f'postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}') # Engine for connect PostgreSQL database\n",
    "    try: # Handle potential errors that might occur during the data loading process\n",
    "        with engine.connect() as connection: # Ensure connection is properly handled and closed after use\n",
    "            connection.execute(text(\"\"\" \n",
    "                CREATE TABLE IF NOT EXISTS pm25_readings (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    timestamp TIMESTAMP,\n",
    "                    region VARCHAR(255),\n",
    "                    pm25_value REAL,\n",
    "                    latitude REAL,\n",
    "                    longitude REAL\n",
    "                )\n",
    "            \"\"\"))\n",
    "            print(\"Table created successfully.\")\n",
    "    except Exception as e: # To catch any exceptions that may occur during the loading process\n",
    "        print(f\"Error creating table: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting the script...\")\n",
    "\n",
    "    verify_database_connection()  # Verify database connection\n",
    "    create_table()  # Ensure the table exists\n",
    "\n",
    "    current_date = START_DATE # Initialize the value of Start_Date\n",
    "    while current_date <= END_DATE: # Iterates over each date from current_date to end_date\n",
    "        date_str = current_date.strftime(\"%Y-%m-%d\") # Fetch data as string in the format of 'YYYY-MM-DD'\n",
    "        print(f\"Fetching data for {date_str}...\")\n",
    "\n",
    "        # Processing retrieved data\n",
    "        items, region_metadata = fetch_pm25_data_for_date(API_URL, date_str)\n",
    "        if items and region_metadata: # Checks if both items and region metadata were successfully retrieved\n",
    "            data_frame = process_items(items, region_metadata) # If data available, processes items and region metadata into pandas DataFrame\n",
    "            if not data_frame.empty: # Checks if the resulting DataFrame is not empty\n",
    "                data_frame['timestamp'] = pd.to_datetime(data_frame['timestamp'])\n",
    "                # Sort the DataFrame by timestamp in chronological order\n",
    "                data_frame = data_frame.sort_values(by='timestamp', ascending=True) # Convert to datetime format\n",
    "                load_data_to_postgres(data_frame)\n",
    "            else: # If DataFrame is empty\n",
    "                print(f\"No data collected for {date_str}.\")\n",
    "        else: # If data fetching fails\n",
    "            print(f\"Failed to fetch data for {date_str}.\")\n",
    "\n",
    "        # Increment the date by one day\n",
    "        current_date += timedelta(days=1) # Increase date by 1 day for next iteration of the loop\n",
    "    \n",
    "    print(\"Script completed.\")\n",
    "\n",
    "if __name__ == \"__main__\": # Running main function\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea70d7",
   "metadata": {},
   "source": [
    "### **PM2.5 Output Result**\n",
    "<img src=\"https://raw.githubusercontent.com/YvonneLipLim/Images/main/PM25_Output.png\" alt=\"Alt Text\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06dbfa-a65c-42bb-8bbb-9ef3109814c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
